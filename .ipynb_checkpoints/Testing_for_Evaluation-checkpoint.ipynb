{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb94d61-c4cd-4822-845b-f8674f1acf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n"
     ]
    }
   ],
   "source": [
    "# ### Tables\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 42\n",
    "\n",
    "# 1. Python built-in random\n",
    "random.seed(SEED)\n",
    "\n",
    "# 2. NumPy\n",
    "np.random.seed(SEED)\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from compare_dataframes import compare_dataframes\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "pd.set_option(\"display.max_rows\", None)  # Don't truncate column values\n",
    "pd.set_option(\"display.max_colwidth\", None)  # Don't truncate column values\n",
    "pd.set_option('display.max_rows', None, 'display.max_columns', None)\n",
    "%xmode Plain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ba83f-f63f-404f-982b-ddc586b7f991",
   "metadata": {},
   "source": [
    "# Testing Generated Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5269b7f6-a8aa-4d63-b990-f7d206eaa697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not parse Experiment_ID from Experiment 1 (#4232)\n",
      "Warning: Could not parse Experiment_ID from Experiment 2 (#4233)\n",
      "Warning: Could not parse Experiment_ID from Experiment 3 (#4234)\n",
      "Warning: Could not parse Experiment_ID from Experiment 4 (#4235)\n",
      "Warning: Could not parse Experiment_ID from Experiment 5 (#4236)\n",
      "Warning: Could not parse Experiment_ID from Experiment 6 (#4237)\n",
      "Warning: Could not parse Experiment_ID from Experiment 7 (#4238)\n",
      "Warning: Could not parse Experiment_ID from Experiment 8 (#4239)\n",
      "Warning: Could not parse Experiment_ID from Experiment 9 (#4240)\n",
      "Warning: Could not parse Experiment_ID from Experiment 10 (#4241)\n",
      "Warning: Could not parse Experiment_ID from Experiment 1 (#4242)\n",
      "Warning: Could not parse Experiment_ID from Experiment 2 (#4243)\n",
      "Warning: Could not parse Experiment_ID from Experiment 3 (#4244)\n",
      "Warning: Could not parse Experiment_ID from Experiment 4 (#4245)\n",
      "Warning: Could not parse Experiment_ID from Experiment 5 (#4246)\n",
      "Warning: Could not parse Experiment_ID from Experiment 6 (#4247)\n",
      "Warning: Could not parse Experiment_ID from Experiment 7 (#4248)\n",
      "Warning: Could not parse Experiment_ID from Experiment 8 (#4249)\n",
      "Warning: Could not parse Experiment_ID from Experiment 9 (#4250)\n",
      "Warning: Could not parse Experiment_ID from Experiment 10 (#4251)\n",
      "Warning: Could not parse Experiment_ID from Experiment 1 (#4252)\n",
      "Warning: Could not parse Experiment_ID from Experiment 2 (#4253)\n",
      "Warning: Could not parse Experiment_ID from Experiment 3 (#4254)\n",
      "Warning: Could not parse Experiment_ID from Experiment 4 (#4255)\n",
      "Warning: Could not parse Experiment_ID from Experiment 5 (#4256)\n",
      "Warning: Could not parse Experiment_ID from Experiment 6 (#4257)\n",
      "Warning: Could not parse Experiment_ID from Experiment 7 (#4258)\n",
      "Warning: Could not parse Experiment_ID from Experiment 8 (#4259)\n",
      "Warning: Could not parse Experiment_ID from Experiment 9 (#4260)\n",
      "Warning: Could not parse Experiment_ID from Experiment 10 (#4261)\n",
      "Warning: Could not parse Experiment_ID from Experiment 1 (#4262)\n",
      "Warning: Could not parse Experiment_ID from Experiment 2 (#4263)\n",
      "Warning: Could not parse Experiment_ID from Experiment 3 (#4264)\n",
      "Warning: Could not parse Experiment_ID from Experiment 4 (#4265)\n",
      "Warning: Could not parse Experiment_ID from Experiment 5 (#4266)\n",
      "Warning: Could not parse Experiment_ID from Experiment 6 (#4267)\n",
      "Warning: Could not parse Experiment_ID from Experiment 7 (#4268)\n",
      "Warning: Could not parse Experiment_ID from Experiment 8 (#4269)\n",
      "Warning: Could not parse Experiment_ID from Experiment 9 (#4270)\n",
      "Warning: Could not parse Experiment_ID from Experiment 10 (#4271)\n",
      "Warning: Could not parse Experiment_ID from Experiment 1 (#4272)\n",
      "Warning: Could not parse Experiment_ID from Experiment 2 (#4273)\n",
      "Warning: Could not parse Experiment_ID from Experiment 3 (#4274)\n",
      "Warning: Could not parse Experiment_ID from Experiment 4 (#4275)\n",
      "Warning: Could not parse Experiment_ID from Experiment 5 (#4276)\n",
      "Warning: Could not parse Experiment_ID from Experiment 6 (#4277)\n",
      "Warning: Could not parse Experiment_ID from Experiment 7 (#4278)\n",
      "Warning: Could not parse Experiment_ID from Experiment 8 (#4279)\n",
      "Warning: Could not parse Experiment_ID from Experiment 9 (#4280)\n",
      "Warning: Could not parse Experiment_ID from Experiment 10 (#4281)\n",
      "Warning: Could not parse Experiment_ID from Experiment 1 (#4292)\n",
      "Warning: Could not parse Experiment_ID from Experiment 2 (#4293)\n",
      "Warning: Could not parse Experiment_ID from Experiment 3 (#4294)\n",
      "Warning: Could not parse Experiment_ID from Experiment 4 (#4295)\n",
      "Warning: Could not parse Experiment_ID from Experiment 5 (#4296)\n",
      "Warning: Could not parse Experiment_ID from Experiment 6 (#4297)\n",
      "Warning: Could not parse Experiment_ID from Experiment 7 (#4298)\n",
      "Warning: Could not parse Experiment_ID from Experiment 8 (#4299)\n",
      "Warning: Could not parse Experiment_ID from Experiment 9 (#4300)\n",
      "Warning: Could not parse Experiment_ID from Experiment 10 (#4301)\n",
      "Warning: Could not parse Experiment_ID from Experiment 1 (#4302)\n",
      "Warning: Could not parse Experiment_ID from Experiment 2 (#4303)\n",
      "Warning: Could not parse Experiment_ID from Experiment 3 (#4304)\n",
      "Warning: Could not parse Experiment_ID from Experiment 4 (#4305)\n",
      "Warning: Could not parse Experiment_ID from Experiment 1 (#4314)\n",
      "Warning: Could not parse Experiment_ID from Experiment 2 (#4315)\n",
      "Warning: Could not parse Experiment_ID from Experiment 3 (#4316)\n",
      "Warning: Could not parse Experiment_ID from Experiment 4 (#4317)\n",
      "Warning: Could not parse Experiment_ID from Experiment 1 (#4322)\n",
      "Warning: Could not parse Experiment_ID from Experiment 2 (#4323)\n",
      "Warning: Could not parse Experiment_ID from Experiment 3 (#4324)\n",
      "Warning: Could not parse Experiment_ID from Experiment 4 (#4325)\n",
      "Warning: Could not parse Experiment_ID from Experiment 1 (#4306)\n",
      "Warning: Could not parse Experiment_ID from Experiment 2 (#4307)\n",
      "Warning: Could not parse Experiment_ID from Experiment 3 (#4308)\n",
      "Warning: Could not parse Experiment_ID from Experiment 4 (#4309)\n",
      "Warning: Could not parse Experiment_ID from Experiment 1 (#4310)\n",
      "Warning: Could not parse Experiment_ID from Experiment 2 (#4311)\n",
      "Warning: Could not parse Experiment_ID from Experiment 3 (#4312)\n",
      "Warning: Could not parse Experiment_ID from Experiment 4 (#4313)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Basis</th>\n",
       "      <th>Matt_Beads</th>\n",
       "      <th>Matt_Silica</th>\n",
       "      <th>Wachs</th>\n",
       "      <th>Verdicker</th>\n",
       "      <th>Time_To_Mix</th>\n",
       "      <th>Time_To_Dispense</th>\n",
       "      <th>thickness</th>\n",
       "      <th>gloss_60</th>\n",
       "      <th>gloss_85</th>\n",
       "      <th>haze</th>\n",
       "      <th>CIELAB_10deg_L</th>\n",
       "      <th>CIELAB_10deg_a</th>\n",
       "      <th>CIELAB_10deg_b</th>\n",
       "      <th>Viskos1</th>\n",
       "      <th>Viskos2</th>\n",
       "      <th>abrasion0_x</th>\n",
       "      <th>abrasion1_x</th>\n",
       "      <th>abrasion2_x</th>\n",
       "      <th>abrasion3_x</th>\n",
       "      <th>RUN_ID</th>\n",
       "      <th>Experiment_ID</th>\n",
       "      <th>abrasion0_y</th>\n",
       "      <th>abrasion1_y</th>\n",
       "      <th>abrasion2_y</th>\n",
       "      <th>abrasion3_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Basis, Matt_Beads, Matt_Silica, Wachs, Verdicker, Time_To_Mix, Time_To_Dispense, thickness, gloss_60, gloss_85, haze, CIELAB_10deg_L, CIELAB_10deg_a, CIELAB_10deg_b, Viskos1, Viskos2, abrasion0_x, abrasion1_x, abrasion2_x, abrasion3_x, RUN_ID, Experiment_ID, abrasion0_y, abrasion1_y, abrasion2_y, abrasion3_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Initialize empty DataFrame for FinalTable with proper data types\n",
    "final_table = pd.DataFrame(columns=[\n",
    "    'RUN_ID', 'Experiment_ID', 'Basis', 'Matt_Beads', 'Matt_Silica', 'Wachs', 'Verdicker',\n",
    "    'Time_To_Mix', 'Time_To_Dispense', 'thickness', 'gloss_60', 'gloss_85', 'haze',\n",
    "    'CIELAB_10deg_L', 'CIELAB_10deg_a', 'CIELAB_10deg_b', 'Viskos1', 'Viskos2',\n",
    "    'abrasion0', 'abrasion1', 'abrasion2', 'abrasion3'\n",
    "])\n",
    "\n",
    "# Load all source files with proper data types\n",
    "run_statistic = pd.read_excel('RunStatistic.xlsx', skiprows=8)\n",
    "viskos_data = pd.read_csv('viskos_means.csv', sep=';')\n",
    "abrasion_data = pd.read_csv('abrasion.csv', sep=';')\n",
    "\n",
    "# Ensure Experiment_ID is consistent type (int) in abrasion_data\n",
    "abrasion_data['Experiment_ID'] = abrasion_data['Experiment_ID'].astype('int64')\n",
    "\n",
    "# Get unique RUN_IDs and Experiment_IDs from RunStatistic\n",
    "unique_runs = run_statistic['Run ID'].unique()\n",
    "\n",
    "for run_id in unique_runs:\n",
    "    run_data = run_statistic[run_statistic['Run ID'] == run_id]\n",
    "    unique_experiments = run_data['Experiment'].unique()\n",
    "    \n",
    "    for experiment in unique_experiments:\n",
    "        # Initialize dictionary for current row with default float values\n",
    "        row_data = {\n",
    "            'RUN_ID': int(run_id),  # Ensure RUN_ID is int\n",
    "            'Experiment_ID': None,\n",
    "            'Basis': np.nan, \n",
    "            'Matt_Beads': np.nan,\n",
    "            'Matt_Silica': np.nan,\n",
    "            'Wachs': np.nan,\n",
    "            'Verdicker': np.nan,\n",
    "            'Time_To_Mix': np.nan,\n",
    "            'Time_To_Dispense': np.nan,\n",
    "            'thickness': np.nan,\n",
    "            'gloss_60': np.nan,\n",
    "            'gloss_85': np.nan,\n",
    "            'haze': np.nan,\n",
    "            'CIELAB_10deg_L': np.nan,\n",
    "            'CIELAB_10deg_a': np.nan,\n",
    "            'CIELAB_10deg_b': np.nan,\n",
    "            'Viskos1': np.nan,\n",
    "            'Viskos2': np.nan,\n",
    "            'abrasion0': np.nan,\n",
    "            'abrasion1': np.nan,\n",
    "            'abrasion2': np.nan,\n",
    "            'abrasion3': np.nan\n",
    "        }\n",
    "        \n",
    "        # Extract Experiment ID and Measurement Number from 'Experiment' column\n",
    "        match = re.match(r'Experiment\\s+(\\d+)\\s*$$#\\d+$$', str(experiment))\n",
    "        if match:\n",
    "            row_data['Experiment_ID'] = int(match.group(1))\n",
    "        \n",
    "        # Skip if we couldn't parse Experiment_ID\n",
    "        if row_data['Experiment_ID'] is None:\n",
    "            print(f\"Warning: Could not parse Experiment_ID from {experiment}\")\n",
    "            continue\n",
    "        \n",
    "        # Get data for this specific run and experiment\n",
    "        exp_data = run_data[run_data['Experiment'] == experiment]\n",
    "        \n",
    "        # Helper function to safely extract dispensed amounts\n",
    "        def get_dispensed_amount(data, material):\n",
    "            try:\n",
    "                amount = data.loc[data['Dispense Material'] == material, 'Dispensed Amount'].values[0]\n",
    "                return float(amount) if not pd.isna(amount) else 0.0\n",
    "            except:\n",
    "                return 0.0\n",
    "        \n",
    "        row_data['Basis'] = get_dispensed_amount(exp_data, 'I2Dach_Hesse_Basismodul')\n",
    "        row_data['Matt_Beads'] = get_dispensed_amount(exp_data, 'I2Dach_Hesse_Mattmodul Beads HM 9-007')\n",
    "        row_data['Matt_Silica'] = get_dispensed_amount(exp_data, 'I2Dach_Hesse_Mattmodul Silica HM 9-008')\n",
    "        row_data['Wachs'] = get_dispensed_amount(exp_data, 'I2Dach_Hesse_Wachsmodul HM 7-004')\n",
    "        row_data['Verdicker'] = get_dispensed_amount(exp_data, 'I2Dach_Hesse_Verdickermodul HZ 3-88')\n",
    "        \n",
    "        # Extract timestamps for time calculations\n",
    "        try:\n",
    "            def get_timestamp(data, node_name):\n",
    "                try:\n",
    "                    ts = data.loc[data['Node'] == node_name, 'Started (UTC)'].iloc[0]\n",
    "                    if pd.isna(ts):\n",
    "                        return None\n",
    "                    return datetime.strptime(str(ts), '%Y.%m.%d %H:%M:%S')\n",
    "                except:\n",
    "                    return None\n",
    "            \n",
    "            dispense_time = get_timestamp(exp_data, '3580: Dispense Liquid - M')\n",
    "            mix_time = get_timestamp(exp_data, '3583: Mix')\n",
    "            drawdown_time = get_timestamp(exp_data, '3585: Draw Down')\n",
    "            \n",
    "            if all(t is not None for t in [dispense_time, mix_time, drawdown_time]):\n",
    "                row_data['Time_To_Mix'] = (mix_time - dispense_time).total_seconds()\n",
    "                row_data['Time_To_Dispense'] = (drawdown_time - mix_time).total_seconds()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error calculating times for run {run_id} experiment {row_data['Experiment_ID']}: {str(e)}\")\n",
    "        \n",
    "        # Find and load characterization files\n",
    "        try:\n",
    "            measurement_dir = f\"measurements/Product A28-R{run_id}-E{row_data['Experiment_ID']}-T0/\"\n",
    "            \n",
    "            # Xrite data (CIELAB values)\n",
    "            xrite_files = glob.glob(os.path.join(measurement_dir, '*xrite*'))\n",
    "            if xrite_files:\n",
    "                try:\n",
    "                    xrite_data = pd.read_csv(xrite_files[0], sep=',')\n",
    "                    if len(xrite_data) > 1:\n",
    "                        row_data['CIELAB_10deg_L'] = float(xrite_data.iloc[1].get('CIELAB_10deg_L', np.nan)) - 4.36\n",
    "                        row_data['CIELAB_10deg_a'] = float(xrite_data.iloc[1].get('CIELAB_10deg_a', np.nan))\n",
    "                        row_data['CIELAB_10deg_b'] = float(xrite_data.iloc[1].get('CIELAB_10deg_b', np.nan))\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error reading xrite data for run {run_id} experiment {row_data['Experiment_ID']}\")\n",
    "            \n",
    "            # Haze-Gloss data\n",
    "            hazegloss_files = glob.glob(os.path.join(measurement_dir, '*haze*'))\n",
    "            if hazegloss_files:\n",
    "                try:\n",
    "                    hazegloss_data = pd.read_csv(hazegloss_files[0], sep=',')\n",
    "                    if len(hazegloss_data) > 1:\n",
    "                        row_data['gloss_60'] = float(hazegloss_data.iloc[1].get('gloss60', np.nan))\n",
    "                        row_data['gloss_85'] = float(hazegloss_data.iloc[1].get('gloss85', np.nan))\n",
    "                        row_data['haze'] = float(hazegloss_data.iloc[1].get('haze', np.nan))\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error reading haze-gloss data for run {run_id} experiment {row_data['Experiment_ID']}\")\n",
    "            \n",
    "            # Minitest data (thickness)\n",
    "            minitest_files = glob.glob(os.path.join(measurement_dir, '*minitest*'))\n",
    "            if minitest_files:\n",
    "                try:\n",
    "                    minitest_data = pd.read_csv(minitest_files[0], header=None, skiprows=1)\n",
    "                    if len(minitest_data) >= 3:\n",
    "                        thickness_value = minitest_data.iloc[2, 0]  # Row 3 (0-based=2), first column\n",
    "                        if not pd.isna(thickness_value):\n",
    "                            row_data['thickness'] = float(thickness_value) - 10\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error reading minitest data for run {run_id} experiment {row_data['Experiment_ID']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error accessing measurement files for run {run_id} experiment {row_data['Experiment_ID']}\")\n",
    "        \n",
    "        # Add viskos data (positional alignment)\n",
    "        current_row_index = len(final_table)\n",
    "        if current_row_index < len(viskos_data):\n",
    "            try:\n",
    "                row_data['Viskos1'] = float(viskos_data.iloc[current_row_index]['Viskos1'])\n",
    "                row_data['Viskos2'] = float(viskos_data.iloc[current_row_index]['Viskos2'])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Append the current row to final table\n",
    "        final_table = pd.concat([final_table, pd.DataFrame([row_data])], ignore_index=True)\n",
    "\n",
    "# Ensure Experiment_ID is consistent type (int) in final_table before merging\n",
    "final_table['Experiment_ID'] = final_table['Experiment_ID'].astype('int64')\n",
    "\n",
    "# Merge with abrasion data (by RUN_ID and Experiment_ID)\n",
    "final_table = pd.merge(\n",
    "    final_table,\n",
    "    abrasion_data,\n",
    "    on=['RUN_ID', 'Experiment_ID'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Display the final table\n",
    "final_table.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0fb5b9-2272-4925-95aa-c57b1878006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f661025-905f-421d-b6c1-94760e67420c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f89e09-486f-4f64-a354-92732494880d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RUN_ID', 'Experiment_ID', 'Basis', 'Matt_Beads', 'Matt_Silica',\n",
       "       'Wachs', 'Verdicker', 'Time_To_Mix', 'Time_To_Dispense', 'thickness',\n",
       "       'gloss_60', 'gloss_85', 'haze', 'CIELAB_10deg_L', 'CIELAB_10deg_a',\n",
       "       'CIELAB_10deg_b', 'Viskos1', 'Viskos2', 'abrasion0', 'abrasion1',\n",
       "       'abrasion2', 'abrasion3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b33a99ec-f91b-4455-86e7-bad7271d4a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ FinalTable columns:\n",
      " ['RUN_ID', 'Experiment_ID', 'Basis', 'Matt_Beads', 'Matt_Silica', 'Wachs', 'Verdicker', 'Time_To_Mix', 'Time_To_Dispense', 'thickness', 'gloss_60', 'gloss_85', 'haze', 'CIELAB_10deg_L', 'CIELAB_10deg_a', 'CIELAB_10deg_b', 'Viskos1', 'Viskos2', 'abrasion0', 'abrasion1', 'abrasion2', 'abrasion3']\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“‹ FinalTable columns:\\n\", final_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be86b09-fb9a-4ea1-a8d1-091f114a57ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that don't match: ['thickness', 'viskos1', 'viskos2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_csv(\"target.csv\", delimiter=\";\")\n",
    "\n",
    "final_df.rename(columns={'RUN_ID': 'run_id', 'Experiment_ID': 'experiment_id'}, inplace=True)\n",
    "final_df = final_df.sort_values(by=['run_id', 'experiment_id'], ascending=[True, True])  # Change to False for descending\n",
    "compare_dataframes(final_df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4578ec3-3098-4de8-ac26-31fb27286f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>basis</th>\n",
       "      <th>matt_beads</th>\n",
       "      <th>matt_silica</th>\n",
       "      <th>wachs</th>\n",
       "      <th>verdicker</th>\n",
       "      <th>time_to_mix</th>\n",
       "      <th>time_to_dispense</th>\n",
       "      <th>thickness</th>\n",
       "      <th>gloss_60</th>\n",
       "      <th>gloss_85</th>\n",
       "      <th>haze</th>\n",
       "      <th>cielab_10deg_l</th>\n",
       "      <th>cielab_10deg_a</th>\n",
       "      <th>cielab_10deg_b</th>\n",
       "      <th>viskos1</th>\n",
       "      <th>viskos2</th>\n",
       "      <th>abrasion0</th>\n",
       "      <th>abrasion1</th>\n",
       "      <th>abrasion2</th>\n",
       "      <th>abrasion3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>841</td>\n",
       "      <td>1</td>\n",
       "      <td>100.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>253.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>38.8</td>\n",
       "      <td>82.4</td>\n",
       "      <td>93.2</td>\n",
       "      <td>257.8</td>\n",
       "      <td>-0.001097</td>\n",
       "      <td>-0.258434</td>\n",
       "      <td>0.144692</td>\n",
       "      <td>220.531</td>\n",
       "      <td>140.237</td>\n",
       "      <td>0.106772</td>\n",
       "      <td>0.119091</td>\n",
       "      <td>0.133310</td>\n",
       "      <td>0.133310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>841</td>\n",
       "      <td>2</td>\n",
       "      <td>85.498</td>\n",
       "      <td>4.987</td>\n",
       "      <td>5.017</td>\n",
       "      <td>2.011</td>\n",
       "      <td>2.505</td>\n",
       "      <td>753.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>35.9</td>\n",
       "      <td>15.9</td>\n",
       "      <td>31.6</td>\n",
       "      <td>116.3</td>\n",
       "      <td>6.127444</td>\n",
       "      <td>-0.305656</td>\n",
       "      <td>-0.805050</td>\n",
       "      <td>581.741</td>\n",
       "      <td>310.688</td>\n",
       "      <td>0.043666</td>\n",
       "      <td>0.036357</td>\n",
       "      <td>0.044164</td>\n",
       "      <td>0.044164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>841</td>\n",
       "      <td>3</td>\n",
       "      <td>82.763</td>\n",
       "      <td>3.731</td>\n",
       "      <td>11.244</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.259</td>\n",
       "      <td>660.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>22.6</td>\n",
       "      <td>67.4</td>\n",
       "      <td>8.745639</td>\n",
       "      <td>-0.275901</td>\n",
       "      <td>-1.196211</td>\n",
       "      <td>240.638</td>\n",
       "      <td>140.959</td>\n",
       "      <td>0.044640</td>\n",
       "      <td>0.059309</td>\n",
       "      <td>0.055542</td>\n",
       "      <td>0.055542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>841</td>\n",
       "      <td>4</td>\n",
       "      <td>88.256</td>\n",
       "      <td>3.766</td>\n",
       "      <td>1.248</td>\n",
       "      <td>3.008</td>\n",
       "      <td>3.746</td>\n",
       "      <td>702.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>39.6</td>\n",
       "      <td>28.9</td>\n",
       "      <td>50.4</td>\n",
       "      <td>229.5</td>\n",
       "      <td>3.954611</td>\n",
       "      <td>-0.313251</td>\n",
       "      <td>-0.612329</td>\n",
       "      <td>864.671</td>\n",
       "      <td>440.999</td>\n",
       "      <td>0.114318</td>\n",
       "      <td>0.097226</td>\n",
       "      <td>0.131349</td>\n",
       "      <td>0.131349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>841</td>\n",
       "      <td>5</td>\n",
       "      <td>87.864</td>\n",
       "      <td>6.564</td>\n",
       "      <td>0.963</td>\n",
       "      <td>1.504</td>\n",
       "      <td>3.155</td>\n",
       "      <td>659.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>47.8</td>\n",
       "      <td>214.1</td>\n",
       "      <td>4.616559</td>\n",
       "      <td>-0.339749</td>\n",
       "      <td>-0.567780</td>\n",
       "      <td>757.979</td>\n",
       "      <td>392.052</td>\n",
       "      <td>0.042172</td>\n",
       "      <td>0.038189</td>\n",
       "      <td>0.064519</td>\n",
       "      <td>0.064519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run_id  experiment_id    basis  matt_beads  matt_silica  wachs  verdicker  \\\n",
       "0     841              1  100.023       0.000        0.000  0.000      0.000   \n",
       "1     841              2   85.498       4.987        5.017  2.011      2.505   \n",
       "2     841              3   82.763       3.731       11.244  1.007      1.259   \n",
       "3     841              4   88.256       3.766        1.248  3.008      3.746   \n",
       "4     841              5   87.864       6.564        0.963  1.504      3.155   \n",
       "\n",
       "   time_to_mix  time_to_dispense  thickness  gloss_60  gloss_85   haze  \\\n",
       "0        253.0             392.0       38.8      82.4      93.2  257.8   \n",
       "1        753.0             321.0       35.9      15.9      31.6  116.3   \n",
       "2        660.0             320.0       34.0      10.7      22.6   67.4   \n",
       "3        702.0             323.0       39.6      28.9      50.4  229.5   \n",
       "4        659.0             318.0       39.8      27.0      47.8  214.1   \n",
       "\n",
       "   cielab_10deg_l  cielab_10deg_a  cielab_10deg_b  viskos1  viskos2  \\\n",
       "0       -0.001097       -0.258434        0.144692  220.531  140.237   \n",
       "1        6.127444       -0.305656       -0.805050  581.741  310.688   \n",
       "2        8.745639       -0.275901       -1.196211  240.638  140.959   \n",
       "3        3.954611       -0.313251       -0.612329  864.671  440.999   \n",
       "4        4.616559       -0.339749       -0.567780  757.979  392.052   \n",
       "\n",
       "   abrasion0  abrasion1  abrasion2  abrasion3  \n",
       "0   0.106772   0.119091   0.133310   0.133310  \n",
       "1   0.043666   0.036357   0.044164   0.044164  \n",
       "2   0.044640   0.059309   0.055542   0.055542  \n",
       "3   0.114318   0.097226   0.131349   0.131349  \n",
       "4   0.042172   0.038189   0.064519   0.064519  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05231af1-0406-4e55-9392-b198e0eca195",
   "metadata": {},
   "source": [
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84050e5b-0062-445d-a954-784e2e23875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc56dda5-5db2-4153-bfa6-b81171767fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run_id',\n",
       " 'experiment_id',\n",
       " 'basis',\n",
       " 'matt_beads',\n",
       " 'matt_silica',\n",
       " 'wachs',\n",
       " 'verdicker',\n",
       " 'time_to_mix',\n",
       " 'time_to_dispense',\n",
       " 'thickness',\n",
       " 'gloss_60',\n",
       " 'gloss_85',\n",
       " 'haze',\n",
       " 'cielab_10deg_l',\n",
       " 'cielab_10deg_a',\n",
       " 'cielab_10deg_b',\n",
       " 'viskos1',\n",
       " 'viskos2',\n",
       " 'abrasion0',\n",
       " 'abrasion1',\n",
       " 'abrasion2',\n",
       " 'abrasion3']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aa6eaa4-dfc3-4ea8-b348-f1b42eee9e1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Basis', 'Matt_Beads', 'Matt_Silica', 'Wachs', 'Verdicker', 'Time_To_Mix', 'Time_To_Dispense'] not in index\"",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  Cell \u001b[92mIn[11]\u001b[39m\u001b[92m, line 146\u001b[39m\n    X_train, X_test, y_train, y_test, y_scaler = preprocess_data(target)\n",
      "  Cell \u001b[92mIn[11]\u001b[39m\u001b[92m, line 34\u001b[39m in \u001b[95mpreprocess_data\u001b[39m\n    X = df[features]\n",
      "  File \u001b[92m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3899\u001b[39m in \u001b[95m__getitem__\u001b[39m\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n",
      "  File \u001b[92m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6115\u001b[39m in \u001b[95m_get_indexer_strict\u001b[39m\n    self._raise_if_missing(keyarr, indexer, axis_name)\n",
      "\u001b[36m  \u001b[39m\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6179\u001b[39m\u001b[36m in \u001b[39m\u001b[35m_raise_if_missing\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mraise KeyError(f\"{not_found} not in index\")\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m\u001b[31m:\u001b[39m \"['Basis', 'Matt_Beads', 'Matt_Silica', 'Wachs', 'Verdicker', 'Time_To_Mix', 'Time_To_Dispense'] not in index\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern, DotProduct, RationalQuadratic\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "# Assuming df is your DataFrame with the given columns\n",
    "# If loading from CSV: df = pd.read_csv('FinalTable.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    # Select relevant features and targets\n",
    "    features = [ 'Basis',\n",
    " 'Matt_Beads',\n",
    " 'Matt_Silica',\n",
    " 'Wachs',\n",
    " 'Verdicker',\n",
    " 'Time_To_Mix',\n",
    " 'Time_To_Dispense',\n",
    " 'thickness']\n",
    "    targets = ['Viskos1', 'gloss_60']\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df[targets]\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, features)\n",
    "        ])\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "    X_test_preprocessed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Scale targets\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "    y_test_scaled = y_scaler.transform(y_test)\n",
    "    \n",
    "    return X_train_preprocessed, X_test_preprocessed, y_train_scaled, y_test_scaled, y_scaler\n",
    "\n",
    "# Model Training and Evaluation\n",
    "def evaluate_gp_models(X_train, X_test, y_train, y_test, y_scaler):\n",
    "    # Define kernel configurations to try\n",
    "    kernels = [\n",
    "        ConstantKernel() * RBF() + ConstantKernel(),  # RBF kernel\n",
    "        ConstantKernel() * Matern(nu=1.5) + ConstantKernel(),  # Matern 3/2\n",
    "        ConstantKernel() * Matern(nu=2.5) + ConstantKernel(),  # Matern 5/2\n",
    "        ConstantKernel() * DotProduct() + ConstantKernel(),  # Linear kernel\n",
    "        ConstantKernel() * RationalQuadratic() + ConstantKernel(),  # Rational Quadratic\n",
    "        ConstantKernel() * RBF() + ConstantKernel() * RBF(),  # Sum of RBF kernels\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    best_model = None\n",
    "    best_score = float('inf')\n",
    "    \n",
    "    for i, kernel in enumerate(kernels):\n",
    "        try:\n",
    "            # Create and train Gaussian Process model\n",
    "            gp = GaussianProcessRegressor(\n",
    "                kernel=kernel,\n",
    "                n_restarts_optimizer=10,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # Use MultiOutputRegressor wrapper for multi-target regression\n",
    "            model = MultiOutputRegressor(gp)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred_scaled = model.predict(X_test)\n",
    "            \n",
    "            # Inverse transform to get predictions in original scale\n",
    "            y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
    "            y_true = y_scaler.inverse_transform(y_test)\n",
    "\n",
    "            mse = mean_squared_error(y_test, y_pred_scaled)\n",
    "            mae = mean_absolute_error(y_test, y_pred_scaled)\n",
    "            mape = mean_absolute_percentage_error(y_test, y_pred_scaled)\n",
    "\n",
    "            print(mse)\n",
    "            print(mae)\n",
    "            print(mape)\n",
    "            \n",
    "            # Calculate metrics for both targets\n",
    "            mse = mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "            mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "            mape = mean_absolute_percentage_error(y_true, y_pred, multioutput='raw_values')\n",
    "            \n",
    "            # Store results\n",
    "            res = {\n",
    "                'kernel': str(kernel),\n",
    "                'mse_viskos1': mse[0],\n",
    "                'mse_gloss': mse[1],\n",
    "                'mae_viskos1': mae[0],\n",
    "                'mae_gloss': mae[1],\n",
    "                'mape_viskos1': mape[0],\n",
    "                'mape_gloss': mape[1],\n",
    "                'model': model\n",
    "            }\n",
    "            results.append(res)\n",
    "            \n",
    "            # Track best model based on combined MSE\n",
    "            current_score = np.mean(mse)\n",
    "            if current_score < best_score:\n",
    "                best_score = current_score\n",
    "                best_model = model\n",
    "                \n",
    "            print(f\"Kernel {i+1}: {str(kernel)}\")\n",
    "            print(f\"Viskos1 - MSE: {mse[0]:.2f}, MAE: {mae[0]:.2f}, MAPE: {mape[0]:.2f}\")\n",
    "            print(f\"Gloss60 - MSE: {mse[1]:.2f}, MAE: {mae[1]:.2f}, MAPE: {mape[1]:.2f}\")\n",
    "            print(\"--------------------------------------------------\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with kernel {i+1}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return results, best_model\n",
    "    \n",
    "# Generate random data for demonstration - REPLACE WITH YOUR ACTUAL DATA\n",
    "np.random.seed(42)\n",
    "data_size = 100\n",
    "\n",
    "# Preprocess data\n",
    "X_train, X_test, y_train, y_test, y_scaler = preprocess_data(target)\n",
    "\n",
    "# Evaluate different GP configurations\n",
    "results, best_model = evaluate_gp_models(X_train, X_test, y_train, y_test, y_scaler)\n",
    "\n",
    "# Print best model results\n",
    "print(\"\\n=== Best Model Performance ===\")\n",
    "best_result = min(results, key=lambda x: np.mean([x['mse_viskos1'], x['mse_gloss']]))\n",
    "print(f\"Kernel: {best_result['kernel']}\")\n",
    "print(f\"Viskos1 - MSE: {best_result['mse_viskos1']:.2f}, MAE: {best_result['mae_viskos1']:.2f}, MAPE: {best_result['mape_viskos1']:.2f}\")\n",
    "print(f\"Gloss60 - MSE: {best_result['mse_gloss']:.2f}, MAE: {best_result['mae_gloss']:.2f}, MAPE: {best_result['mape_gloss']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47fe0fc-2d7f-439f-a3a1-d41dee2c043e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
